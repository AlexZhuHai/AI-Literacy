行，我们开始。
今天我们要聊的话题是现在最火的——生成式 AI，也就是大家都在用的 ChatGPT、Kimi 这种大语言模型（LLM）。
很多同学问我，说老师，这东西是不是真有意识了？它是不是要把人类灭绝了？
其实没那么玄乎。我们要祛魅。

首先，咱们得搞清楚核心概念。
什么是大模型？简单说，它就是一个“文字接龙”的高手。
不管是 GPT-4 还是别的，本质上都是基于概率预测下一个字的。
比如我说“白日依山尽”，你脑子里自动跳出“黄河入海流”，对吧？模型也是这么干的。
但是，它读的书比你多得多。它读了互联网上几乎所有的文本。
它的底层核心架构叫 Transformer。这是一个谷歌在2017年提出的架构。
以前的 AI 记性不好，看了后面忘前面。Transformer 有个核心机制叫“注意力机制（Self-Attention）”。
这个机制很厉害，比如一句话“苹果很好吃，因为它是刚摘的”，这个“它”是指苹果。
如果是另一句“苹果手机很贵，因为它配置高”，这个“它”指手机。
Transformer 能算出这个词和句子里其他所有词的关联权重，这就是它“理解”语境的秘密。

然后，很多同学对它是怎么训练出来的特别感兴趣。
这个过程其实很像人类的学习，比较复杂，我稍微展开讲讲，这里大家记一下，后面考试可能会考流程。
大模型的诞生主要分三个阶段，这是个流水线。
第一阶段叫“预训练（Pre-training）”。
就是把海量的书、网页、代码全喂给它。
这时候它就像个博览群书但不懂规矩的野孩子。
它学会了语法，学会了世界知识，但你问它“如何通过考试？”，它可能给你续写一篇小说，因为它不懂这是个问答。
这个阶段消耗算力最大，通常需要几千张显卡跑几个月。
第二阶段，叫“有监督微调（SFT）”。
这时候我们得教它规矩。我们会人工编写很多高质量的问答对。
比如问“1+1等于几”，答“等于2”。
通过这个阶段，模型学会了“指令遵循”，知道你是要它干活，而不是瞎聊。
第三阶段，也是最关键的，叫“人类反馈强化学习（RLHF）”。
这名字挺长哈。简单说，就是让模型生成好几个答案，然后人类（标注员）去给这几个答案打分排序。
比如答案 A 很有礼貌但只有一句，答案 B 很详细但有点粗鲁，人类会告诉模型 B 更好或者 A 更好。
模型通过这个反馈，调整自己的参数，让自己的回答更符合人类的价值观——安全、有用、无害。
这三步走完，一个可用的 ChatGPT 就出来了。

再聊聊局限性吧。
现在 AI 有个大毛病叫“幻觉（Hallucination）”。
就是一本正经地胡说八道。
为什么会这样？因为它是概率模型，不是数据库。
它不是去查资料，它是根据概率“猜”出来的。如果它没见过某个知识，为了把句子接下去，它可能会编造事实。
比如你问它“林黛玉倒拔垂杨柳的情节”，它可能真给你编一段。
所以，大家在用 AI 辅助论文或者工作的时候，一定要进行“事实核查”。

最后，我想说一下未来的趋势。
现在的 AI 已经开始从单纯的文本，走向多模态了。
能看图、能听声音、能画画。
而且正在向 Agent（智能体）发展。
以前是我们问它答，以后可能是你给它个目标：“帮我策划一次去日本的旅行并订好票”，它能自动去调用搜索工具、订票接口，把事儿全办了。
这就是具身智能的前奏。

总结一下今天讲的。
大模型本质是概率预测，底层是 Transformer。
训练过程分三步：预训练、微调、RLHF。
使用时要注意它的幻觉问题。
未来是多模态和智能体的时代。
好，今天就讲到这。